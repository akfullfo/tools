#!/usr/bin/env python3
# ________________________________________________________________________
#
#  Copyright (C) 2020 Andrew Fullford
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
# ________________________________________________________________________
#

import os
import sys
import time
import signal
import json
import argparse
import subprocess
from collections import deque as Queue
from taskforce import poll as Poll
# import multiprocessing
# import traceback
# import inspect
import logging

DEF_THRESHOLD = -0.1
DEF_LIMIT = 3
DEF_FROM_EXT = 'm4a'
DEF_STYLE = 'loudnorm'

#  Supported styles.  The value indicates whether the Workload
#  object should be instantiated per audio file, or per
#  directory containing audio files (ie ablums).
#
SUPPORTED_STYLES = {
        DEF_STYLE: False,
        'track': False,
        'album': True,
    }

#  After this time, unterminated children will be sent SIGKILL.  The time is the
#  number of seconds to wait after the last successful reap before giving up.
#
TERMINATE_WAIT = 3

#  After this much idle time, report activity
#
IDLE_SECS = 1

errors = 0

logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', datefmt="%Y-%m-%d %H:%M:%S")
log = logging.getLogger()
_logfd = None
if log.handlers:
    if hasattr(log.handlers[0], 'stream'):
        _logfd = log.handlers[0].stream.fileno()
    elif hasattr(log.handlers[0], 'socket'):
        _logfd = log.handlers[0].socket


def arg_parse():
    program = os.path.basename(os.path.splitext(sys.argv[0])[0])

    def positive_int(val):
        try:
            a = int(val)
            if a > 0:
                return a
        except:
            pass
        raise argparse.ArgumentTypeError("Value %r is not a positive integer" % val)

    def decibel(val):
        try:
            a = float(val)
            if a <= 0.0:
                return a
        except:
            pass
        raise argparse.ArgumentTypeError("Value %r is not a negative number" % val)

    p = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,
                                description="""
Run ffmpeg to normalize audio file volume.

This article proposes 'album' as the least intrusive adjustment, describing loudnorm as
evil and immoral:

    https://whoislewys.com/2018/10/18/audio_norm_guides_are_evil_and_immoral/

However, track normalization either by directly altering dB or by applying the ffmpeg
'loudnorm' filter is useful in at least one specific case, which is when the audio is to
be played in a noisy environment, specially in a car.

So the program provides three styles of normalization:

    loudnorm
    track
    album

'loudnorm' uses the two-pass approach described here:

    http://peterforgacs.github.io/2018/05/20/Audio-normalization-with-ffmpeg/

to adjust the audio according to the ebu R128 standard.

'track' uses multiple passes to adjust dB towards 0.0 on a track-by-track basis.

'album' uses a two-pass approach.  One finds the loudest track in a directory
of tracks, and then adjusts all tracks by the same amount to move the loudest
track to 0.0 dB.

For example:

    %s -S album -f m4a ~/Music/iTunes

    """ % program)

    p.add_argument('-v', '--verbose', action='store_true', help="List each resulting path")
    p.add_argument('-n', '--dryrun', action='store_true', help="List each resulting path but don't perform the operation")
    p.add_argument('-S', '--style', action='store', default='loudnorm', choices=sorted(SUPPORTED_STYLES),
                    help="Conversion style, default %r" % DEF_STYLE)
    p.add_argument('-f', '--from-ext', action='store', metavar='ext', default=DEF_FROM_EXT,
                    help='"From" extension (without the dot), default %s' % DEF_FROM_EXT)
    p.add_argument('-t', '--threshold', action='store', metavar='-dB', type=decibel, default=DEF_THRESHOLD,
                    help='dB threshold below which volume is adjusted, default %.2f' % DEF_THRESHOLD)
    p.add_argument('-l', '--limit', action='store', metavar='num', default=DEF_LIMIT, type=positive_int,
                    help='Limit on passes to adjust volume, default %r' % DEF_LIMIT)
    p.add_argument('-s', '--simultaneous-tasks', action='store', metavar='num', type=positive_int, default=os.cpu_count(),
                    help='Number of simultaneous tasks to run.  '
                         'Default is the number of CPU threads which is %r on this system' % os.cpu_count())
    p.add_argument('path', nargs='+', action='store', help='Path or paths to descend')

    return p.parse_args()


def error(fmt, *fargs, **kargs):
    global errors
    errors += 1
    name = kargs.pop('name', None)
    exc = kargs.pop('exc', None)

    try:
        msg = fmt % fargs
    except Exception as e:
        msg = "Format of %r %% %r failed -- %s" % (fmt, fargs, e)

    if name:
        log.error("%s: %s", name, msg, exc_info=True)
    else:
        log.error("%s", msg, exc_info=True)
    if hasattr(exc, 'output'):
        log.error("Output ...")
        log.error(exc.output)
    exit(99)


class Workload(object):
    """

        Workload to be run as a separate process.  The class is instantiated in the parent
        process, normally just before it is queued.  The actual processing will occur when
        the object is called (via the __call__ method).

        The object is called with a single argument, which is a Python logging instance.
        All other input and config must be provided via keyword args during
        instantitation.

        This method should return a JSON-serializable object containing the results of the
        workload.  Stdin, stdout, and stderr will be attached to os.devnull.  Any error
        or status reporting must be done via the log instance or embedded in the returned
        object.
    """
    def __init__(self, **args):
        self.args = args
        self.name = None
        self.path = args['path']
        self.limit = args.get('limit', DEF_LIMIT)
        self.threshold = args.get('threshold', DEF_THRESHOLD)
        parts = self.path.split(os.path.sep)
        if len(parts) > 3:
            self.name = os.path.sep.join([parts[0], '...'] + parts[-2:])
        else:
            self.name = self.path
        self.start = None
        self.count = 0
        self.from_ext = '.' + self.args['from_ext']
        self.from_len = len(self.from_ext)

        if not hasattr(self, args['style']):
            raise Exception("Attempt to process with unsupported style %r" % args['style'])
        self._process = getattr(self, args['style'])

        #  Check for accessibility early
        #
        with open(self.path, 'rb'):
            pass

    def __str__(self):
        return self.name

    def __call__(self):
        return self._process()

    def error(self, fmt, *fargs, **kwargs):
        kwargs['name'] = self.name
        error(fmt, *fargs, **kwargs)

    def run(self, cmd):
        return subprocess.check_output(cmd, universal_newlines=True, stderr=subprocess.STDOUT, stdin=subprocess.DEVNULL)

    def get_db(self):
        out = self.run(['ffmpeg', '-i', self.path, '-af', 'volumedetect', '-vn', '-sn', '-dn', '-f', 'null', '/dev/null'])

        max_db = None
        for line in out.splitlines():
            if 'max_volume:' in line:
                line = line.rstrip()
                max_db, db_name = line.split()[-2:]
                if db_name != 'dB':
                    self.error("Bad max volume line: %s", line)
                else:
                    max_db = float(max_db)
                break
        if max_db is None:
            self.error("No max_volume found")
            max_db = 0.0
        elif max_db > 0.01:
            self.error("Detected positive dB %.1f", max_db)
            max_db = 0.0
        return max_db

    def track(self):
        if self.args.get('dryrun'):
            return {"cycles": None}
        tmp = None
        runs = 0
        try:
            max_db = self.get_db()
            for attempt in range(self.limit):
                if max_db < self.threshold:
                    tmp = self.path[:-self.from_len] + '.tmp' + self.from_ext
                    self.run(['ffmpeg', '-i', self.path, '-af', 'volume=%.1fdB' % -max_db, '-c:v', 'copy', tmp])
                    runs += 1
                    os.rename(tmp, self.path)
                    prev_db = max_db
                    max_db = self.get_db()
                if max_db >= self.threshold:
                    break
                if abs(max_db - prev_db) < 0.01:
                    log.info("%s: No significant change in dB %.2f after %r cycles", self.name, max_db, runs)
                    break
            if max_db < self.threshold and runs == self.limit:
                log.warning("%s: Max dB %.2f still below threshold %.2f after %r attempt%s",
                            self.name, max_db, self.threshold, runs, '' if runs == 1 else 's')
            log.debug("%s: %.1f dB vol change (adj %d)", self.name, -max_db, runs)
            return {"cycles": runs}
        except Exception:
            raise
        finally:
            if tmp and os.path.exists(tmp):
                os.unlink(tmp)
                log.warning("Removed %s", tmp)

    def loudnorm(self):
        std_loudnorm = 'loudnorm=I=-23:LRA=7:tp=-2:print_format=json'
        out = self.run(['ffmpeg', '-i', self.path, '-af', std_loudnorm, '-f', 'null', '/dev/null'])

        #  This is ugly.  We need to parse the verbose output until we get to the
        #  lump of JSON we want at the end.
        #
        json_text = None
        try:
            for line in out.splitlines():
                if json_text is not None:
                    line = line.rstrip()
                    if not json_text:
                        if line == '{':
                            json_text += line
                        else:
                            raise Exception("Could not find start of JSON in first-pass output")
                    else:
                        json_text += line
                elif line.startswith("[Parsed_loudnorm"):
                    json_text = ''
            if not json_text:
                raise Exception("No JSON result from first-pass output")

            info = json.loads(json_text)
            adjustment = "%s:linear=true:measured_I=%s:measured_TP=%s:measured_LRA=%s:measured_thresh=%s:offset=%s" % (
                            std_loudnorm,
                            info["input_i"], info["input_tp"], info["input_lra"],
                            info["input_thresh"], info["target_offset"],
                        )
            tmp = None
            tmp = self.path[:-self.from_len] + '.tmp' + self.from_ext
            self.run(['ffmpeg', '-i', self.path, '-af', adjustment, '-c:v', 'copy', tmp])
            os.rename(tmp, self.path)
            log.debug("%s: I:%s TP:%s LRA:%s", self.name, info["input_i"], info["input_tp"], info["input_lra"])
            return {"adjustment": adjustment}
        except Exception:
            raise
        finally:
            if tmp and os.path.exists(tmp):
                os.unlink(tmp)
                log.warning("Removed %s", tmp)


class Task(object):
    """
        Wrapping to manage an individual workload.

        This provides the necessary state and invocation
        to run a workload and collect its result.

        The workload must be callable (via a __call__() method
        and this method must return a JSON-serializable object.
    """
    def __init__(self, workload):
        self.workload = workload
        self.fd = None
        self.pid = None
        self.code = None
        self.signal = None
        self.started = None
        self.duration = None
        self.result = None
        self._json_bytes = b''

    def __str__(self):
        return 'Task(' + str(self.workload) + ')'

    def fileno(self):
        return self.fd

    def _closeall(self, exclude=set()):
        for fd in range(100):
            if fd not in exclude:
                try:
                    os.close(fd)
                except:
                    pass

    def start(self):
        fr_child, to_parent = os.pipe()
        pid = os.fork()
        if pid:
            self.pid = pid
            self.fd = fr_child
            self.started = time.time()
            os.close(to_parent)
            return

        self._closeall({to_parent, _logfd})
        nullfd = os.open(os.devnull, os.O_RDWR)
        os.dup2(nullfd, 1)
        if _logfd != 2:
            os.dup2(nullfd, 2)

        code = 0
        try:
            resp = self.workload()
        except Exception as e:
            resp = {'error': str(e)}
            code = 2
        try:
            json_bytes = (json.dumps(resp) + os.linesep).encode('utf-8')
            os.write(to_parent, json_bytes)
        except Exception as e:
            log.error("%s: Failed to write response -- %s", self, e)
            code = 3
        os._exit(code)

    def finalize(self):
        #  To avoid blocking, this should be called only when input is available.
        #
        buf = os.read(self.fd, 102400)
        if buf:
            self._json_bytes += buf
            return False
        os.close(self.fd)
        self.result = json.loads(self._json_bytes.decode('utf-8'))
        self.duration = time.time() - self.started
        self._json_bytes = None
        return True


class Pool(object):
    """
        Manage a group of Tasks.

        Tasks may be pending, running, exiting, or completed.

        Exiting tasks have closed their reporting file descriptor but have not yet been
        reaped for an exit code.

        Once all tasks have been added to a Pool instance, processing is started by
        calling it(via the __call__() method).

        Processing stops when the pending, running, and exiting queues are empty.  At that
        point, all results are available and can be access by iterating over the Pool
        instance or accessing as a list.

        Premature termination can be triggered with a call to the terminate() method, which
        would typically be called from a signal handler as control otherwise remains in
        the instance call.
    """
    def __init__(self, run_limit=None, log=None):
        self.run_limit = os.cpu_count() if run_limit is None else run_limit
        self.pending = Queue()
        self.running = Queue()
        self.exiting = Queue()
        self.completed = Queue()
        self.pset = Poll.poll()

    #  Support operation as a context manager
    #
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        if exc_type:
            self.terminate()

    #  Support operation as a sequence.  The
    #  sequence operates over the completed task list.
    #
    def __iter__(self):
        for task in self.completed:
            yield task

    def __len__(self):
        return len(self.completed)

    def __getitem__(self, key):
        if isinstance(key, slice):
            return [self.completed[x] for x in range(*key.indices(len(self.completed)))]
        elif isinstance(key, int):
            if key < 0:
                key += len(self.completed)
            if key < 0 or key >= len(self.completed):
                raise IndexError("Index %r is out of range" % key)
            else:
                return self.completed[key]
        else:
            raise TypeError("Invalid type for argument %r" % key)

    def append(self, workload):
        self.pending.append(Task(workload))

    def extend(self, workload_list):
        for workload in workload_list:
            self.append(workload)

    def _reap(self):
        reaped = 0
        for task in list(self.exiting):
            tpid, code = os.waitpid(task.pid, os.WNOHANG)
            if tpid:
                task.code = code
                self.exiting.remove(task)
                self.completed.append(task)
                reaped += 1
        return reaped

    def _reap_all(self):
        #  Reap all exiting tasks for up to TERMINATE_WAIT secs
        #
        toolong = time.time() + TERMINATE_WAIT
        while self.exiting and time.time() < toolong:
            while self._reap():
                continue
            time.sleep(0.1)

    def terminate(self):
        start = time.time()

        #  Empty the pending queue into the completed queue
        #  The code and pid remaing None indicating the task
        #  never ran.
        #
        while self.pending:
            self.completed.append(self.pending.popleft())

        running_total = len(self.running) + len(self.exiting)

        #  If there are any exiting tasks, send a SIGTERM to hurry them up
        #
        for task in self.exiting:
            task.signal = signal.SIGTERM
            os.kill(task.signal, task.pid)

        #  SIGTERM all running tasks and add them to the exiting list
        #
        while self.running:
            task = self.running.popleft()
            task.signal = signal.SIGTERM
            self.exiting.append(task)
            os.kill(task.signal, task.pid)

        self._reap_all()

        #  If any tasks are still running, issue SIGKILL
        #
        if self.exiting:
            hung_total = len(self.exiting)
            log.info("%r of %r task%s failed to terminate after %s secs, escalating",
                            hung_total, running_total, '' if running_total == 1 else 's')
            for task in self.exiting:
                task.signal = signal.SIGKILL
                os.kill(task.signal, task.pid)
            self._reap_all()
            if self.exiting:
                log.error("%r of %r hung task%s failed to exit on SIGKILL after %r secs",
                                len(self.exiting), hung_total, '' if hung_total == 1 else 's', TERMINATE_WAIT)
        else:
            log.info("All %r task%s were terminated normally in %.1f secs",
                            running_total, '' if running_total == 1 else 's', time.time() - start)

    def __call__(self):
        pending_total = len(self.pending)
        log.info("Starting with %r pending task%s", pending_total, '' if pending_total == 1 else 's')

        changes = False
        while self.pending or self.running:
            while self.pending and len(self.running) < self.run_limit:
                task = self.pending.popleft()
                task.start()
                self.pset.register(task, Poll.POLLIN)
                self.running.append(task)
                changes = True
                log.debug("Started %s", task.workload)

            self._reap()

            ev_list = self.pset.poll(IDLE_SECS * 1000)
            if ev_list:
                for task, ev in ev_list:
                    if task.finalize():
                        self.running.remove(task)
                        self.pset.unregister(task)
                        self.exiting.append(task)
                        changes = True
            elif changes:
                log.info("%r pending, %r running, %r exiting, %r completed",
                                len(self.pending), len(self.running), len(self.exiting), len(self.completed))
                changes = False

        log.info("Processing completed, waiting for %r to exit", len(self.exiting))
        self._reap_all()
        if self.exiting:
            log.error("%r task%s did not exit after final wait", len(self.exiting), '' if len(self.exiting) == 1 else 's')


if __name__ == '__main__':
    args = arg_parse()

    tmp = None
    if args.verbose:
        log.setLevel(logging.INFO)

    log.info("Allowing %d simultaneous tasks", args.simultaneous_tasks)
    start_time = time.time()

    results = {}

    with Pool(args.simultaneous_tasks) as pool:
        vargs = vars(args).copy()
        try:
            for path in args.path:
                if os.path.isdir(path):
                    log.info("Scanning %s", path)
                    for dirpath, dirnames, filenames in os.walk(path, onerror=error, followlinks=True):
                        for fname in filenames:
                            if fname.endswith('.' + args.from_ext):
                                vargs['path'] = os.path.join(dirpath, fname)
                                log.debug("Found %s", vargs['path'])
                                pool.append(Workload(**vargs))
                else:
                    log.info("File %s", path)
                    vargs['path'] = path
                    pool.append(Workload(**vargs))
        except Exception as e:
            error("Exception: %s", e, exc=e)

        pool()
        wallclock = time.time() - start_time
        exectime = 0
        for task in pool:
            exectime += task.duration
            if 'error' in task.result:
                log.error("%s -- %s", task.workload, task.result['error'])
        log.info("Wallclock %.1f secs, execution time %.1f secs, speedup %.1f", wallclock, exectime, exectime / wallclock)
